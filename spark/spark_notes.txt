Apache Spark has the following main components:
 - Spark Core and RDDs: provides basic I/O, distributed task dispatching and scheduling.
 - Spark SQL: component lying on top of Spark Core, introduces SchemaRDD. Supports SQL with ODBC/JDBC server and command-line interfaces.
 - Spark Streaming: it leverages the fast scheduling capability of Spark Core, ingests data in small batches, and performs RDD transformations on them.
 - MLlib: distributed machine learning framework on top of Spark.
 
 
PySpark - Python API for Spark
https://sparkbyexamples.com/pyspark-tutorial/

Spark has a master-slave architecture, so whenever we run a Spark application, the master (Spark Driver) creates a "context" which is the entrypoint to the application, and all the actual operations (transformations and actions) are carried out on the worker nodes. The resources are managed by the Cluster Manager.

	   Spark Driver		 =====> Cluster Manager =====> 	Worker Node
  (creates Spark context)								task	task
  
  
There are different cluster managers that can be used:
Standalone, Apache Mesos, Hadoop YARN (most widely used) and Kubernetes

=> If we use "local" as cluster manager to run Spark on our laptops.

=====================================================================================================================================================================

Spark Deploy modes - Client vs Cluster

- Spark deployment modes specify where the driver program of the Spark application should run.
	-> Client: 
	   - the driver will run on the machine where we are submitting the application from. This is used for interactive and debugging purposes.
	   - If we run it from our laptops, then the driver will run on our laptop.
	   - if we close the terminal, then the application also stops.
	   
	-> Cluster:
	   - the driver will run on one of the worker nodes and this node shows as a driver on the Spark Web UI of the application.
	   - This can be used for production purposes.
	   
- To know the deployment mode of an application, we can access the history server UI and in the 'environment' tab, check for spark.submit.deployMode.

=====================================================================================================================================================================

Setting up Spark:
1. Install Java
	sudo amazon-linux-extras install java-openjdk11
	
2. Install Scala.
	curl -fL https://github.com/coursier/launchers/raw/master/cs-x86_64-pc-linux.gz | gzip -d > cs && chmod +x cs && ./cs setup
	
3. Install anaconda
	Prerequisites: yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver
	
	Installation steps: https://docs.anaconda.com/anaconda/install/linux/
	
	wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
	bash Anaconda3-2022.05-Linux-x86_64.sh
	Approve the steps and provide "yes" when asked about init.
	
4. Download and install Spark
	wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
	tar -zxvf spark-3.2.1-bin-hadoop3.2.tgz

	export SPARK_HOME=/home/ec2-user/spark-3.2.1-bin-hadoop3.2
	export PATH=$SPARK_HOME/bin:$PATH
	export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
	
When setting up Jupyter notebook on EC2, while running the "jupyter notebook" command, include a parameter "--ip=<EC2-public IP>".
The best process in my opinion is to include this in a "start_jupyter.sh" script. 
The above comands to set environment variables can also be included in this startup script.

To startup spark, after setting the environment variables, type "pyspark" in the terminal.
Spark also provides a web GUI, which can be accessed at port 4040 or 4041.

Setting up history server for Spark:
The logs location can be configured in SPARK_HOME/conf/spark-defaults.conf. 
Whatever directory we give here needs to be created manually, otherwise it will throw a FileNotFoundException.
A sample configuration is in the github repo.
To start the history server, run $SPARK_HOME/sbin/start-history-server.sh
Visit <url>:18080 to see the history server UI.


To setup master-worker:

 1. On the master instance
	-> go to $SPARK_HOME/conf/spark-env.sh or create one from spark-env.sh.template
	-> In this file, set the SPARK_MASTER_HOST=<ip address of the master>
	-> go to $SPARK_HOME/sbin/ and run start-master.sh
	-> Master UI should be available at 8080.
	
 2. On the worker instances:
	-> go to $SPARK_HOME/conf/spark-env.sh or create one from spark-env.sh.template
	-> In this file, set the SPARK_MASTER_HOST=<ip address of the master>
	-> go to $SPARK_HOME/sbin and run:  ./start-slave.sh spark://<ip-address-of-master>:7077
	
 The security group of the master needs to allow inbound TCP on ports 7077, AND  - one other port - for the worker to connect to the driver.
 This driver port needs to be configured and made to be fixed - so that we only need to open up that one port on the master sg.
 But until we know how to do that, open up ALL INBOUND TCP traffic for the master from the worker-sg.
 
 ============================================================================================================================================================
|| MY THEORY ON MASTER-WORKER CONNECTIVITY: The workers must be polling the master for any jobs. I make this conclusion because I didn't open the worker	||
|| application's port in the worker-sg. Even without this, the master and worker are able to communicate. This implies that the worker initiates the traffic||
|| to the master - and so the return traffic does not need to be specifically whitelisted on the security group.											||
 ============================================================================================================================================================
 
 
=====================================================================================================================================================================

Resilient Distributed Dataset (RDD): https://sparkbyexamples.com/pyspark-rdd/
-> the basic building block of PySpark which is fault-tolerant, immutable distributed collection of objects.
-> RDD divides the data among the different nodes available - so parallelism is really good.
-> some of the basic operations on an RDD are: filter(), map(), persist() and pair RDD functions such as groupByKey(), join().
-> Lazy evaluation: PySpark does not evaluate the transformations and instead it keeps all the transformations till it sees the first RDD action.
-> By default, RDD partitions the elements to the number of cores available.

There are two ways to create an RDD:
	1. parallelizing an existing collection
		for e.g., create an RDD from a Python list (as we did in the first PySpark script).
		
	2. Referencing a dataset in an external storage system (HDFS, S3 etc)
	
Code level instructions are found in the GitHub repo.

Transformations and Actions

- transformations are all evaluated lazily, so they are not actually executed until an Action is encountered.
- Actions return the values from an RDD to a driver program. Any RDD function that returns non-RDD is considered an action.

Examples for transformations: map(), flatMap(), reduceByKey(), sortByKey(), filter().
Examples for actions: 
count(), first(), max(), 
reduce(), // accepts a function with which we can reduce all the records to a single - we can use this for count or sum
take(),   // Returns the record specified as an argument.
collect() // careful when using this with an RDD that contains huge amounts of data as we may run out of memory on the driver.
saveAsTextFile()

Shuffle operations:
 - shuffle is a mechanism used to redistribute the data across different executors and even across machines.
 - PySpark shuffling triggers when we perform certain transformation operations like groupByKey(), reduceByKey(), join() on RDDs.
 - shuffle is an expensive operation, because it needs disk I/O, data serialization & deserialization AND network I/O.
 
When we perform reduceByKey() operation, PySpark does the following

1. PySpark first runs map tasks on all partitions which groups all values for a single key.
2. The results of the map tasks are kept in memory.
3. When results do not fit in memory, PySpark stores the data into a disk.
4. PySpark shuffles the mapped data across partitions, some times it also stores the shuffled data into a disk for reuse when it needs to recalculate.
5. Run the garbage collection
6. Finally runs reduce tasks on each partition based on key.

Having too many partitions for very few data is not good since we can end up with many partitioned files with less number of records in each partition, which results in many tasks to process less data.
Vice versa, when we have too much data and very few partitions, it will result in longer running tasks and some times out of memory errors.

 ========================================================================================================================
||	Getting the right num of partitions is tricky: In case of performance issues in Spark, check the partitioning.		||
 ========================================================================================================================
 
 
Persistence and caching: https://blog.knoldus.com/understanding-persistence-in-apache-spark/

 - To store the results after transformations on an RDD, we can make use of caching and persistence.
 - With caching, it will always be in-memory of the worker nodes, whereas with persistence, we can store it in MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER,
   MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2
   
 - to understand the advantage of persistence, consider the example:
		transformation			transformation
   RDD1 ===============> RDD2 ===================> RDD3
   
   Now each time we do any kind of transformation on RDD3, RDD1 and RDD2 needs to be recomputed again.
   But, if we had cached RDD3, it wouldn't need to recompute this all over.
   
 - Persistence levels:
	Based on storage format:
   1. MEMORY_ONLY			: everything in memory
   2. DISK_ONLY				: everything in disk. useful if I/O to read/write this from disk is less expensive than recomputing
   3. MEMORY_AND_DISK		: Stores partitions in disk which do not fit in-memory. Also called spilling.
							  The data stored in disk is at /tmp.
							  
							  
	Based on memory format:
	
	Here serialized means storing in binary format. This is more space-efficient, but has the added cost of deserializing the data.
	
   4. MEMORY_ONLY_SER		: serialized in-memory.
   5. MEMORY_AND_DISK_SER	: serialized in-memory and disk.
   
   Based on partition replication:
   These options stores a replicated copy of the RDD into some other Worker Node’s cache memory as well.
   Replicated data on the disk will be used to recreate the partition i.e. it helps to recompute the RDD if the other worker node goes down.
   
   6. DISK_ONLY_2
   7. MEMORY_AND_DISK_2
   8. MEMORY_ONLY_2
   9. MEMORY_AND_DISK_SER_2
   10.MEMORY_ONLY_SER_2
   
   
 - PySpark monitors all persist() and cache() calls that we make. It drops persisted data if not used or using a Least-Recently-Used algorithm.
 - We can also manually remove using unpersist().
 - unpersist() removes all blocks from memory and disk.
 
 
Shared variables:
 - Shared variables are mainly used for sharing reused data across tasks/executors.
 - There are two types:
	1. Broadcast variables:
		-> Broadcast to all nodes once they access it. (cached on the machines)
		-> cannot be modified by the tasks, only look - no touch! :)
		
	2. Accumulator variables
		-> used for any accumulating operations in the executors, like count/sum.
		-> the value of accumulator variables cannot be accessed by the worker nodes (only update possible - no read access)
		-> this allows for parallelization across worker nodes - yet to see how this works.
		-> value can be accessed by the driver only.
		