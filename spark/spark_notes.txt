Apache Spark has the following main components:
 - Spark Core and RDDs: provides basic I/O, distributed task dispatching and scheduling.
 - Spark SQL: component lying on top of Spark Core, introduces SchemaRDD. Supports SQL with ODBC/JDBC server and command-line interfaces.
 - Spark Streaming: it leverages the fast scheduling capability of Spark Core, ingests data in small batches, and performs RDD transformations on them.
 - MLlib: distributed machine learning framework on top of Spark.
 
 
PySpark - Python API for Spark
https://sparkbyexamples.com/pyspark-tutorial/

Spark has a master-slave architecture, so whenever we run a Spark application, the master (Spark Driver) creates a "context" which is the entrypoint to the application, and all the actual operations (transformations and actions) are carried out on the worker nodes. The resources are managed by the Cluster Manager.

	   Spark Driver		 =====> Cluster Manager =====> 	Worker Node
  (creates Spark context)								task	task
  
  
There are different cluster managers that can be used:
Standalone, Apache Mesos, Hadoop YARN (most widely used) and Kubernetes

=> If we use "local" as cluster manager to run Spark on our laptops.

=====================================================================================================================================================================

Setting up Spark:
1. Install Java
	sudo amazon-linux-extras install java-openjdk11
	
2. Install Scala.
	curl -fL https://github.com/coursier/launchers/raw/master/cs-x86_64-pc-linux.gz | gzip -d > cs && chmod +x cs && ./cs setup
	
3. Install anaconda
	Prerequisites: yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver
	
	Installation steps: https://docs.anaconda.com/anaconda/install/linux/
	
	wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
	bash Anaconda3-2022.05-Linux-x86_64.sh
	Approve the steps and provide "yes" when asked about init.
	
4. Download and install Spark
	wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
	tar -zxvf spark-3.2.1-bin-hadoop3.2.tgz

	export SPARK_HOME=/home/ec2-user/spark-3.2.1-bin-hadoop3.2
	export PATH=$SPARK_HOME/bin:$PATH
	export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
	
When setting up Jupyter notebook on EC2, while running the "jupyter notebook" command, include a parameter "--ip=<EC2-public IP>".
The best process in my opinion is to include this in a "start_jupyter.sh" script. 
The above comands to set environment variables can also be included in this startup script.

To startup spark, after setting the environment variables, type "pyspark" in the terminal.
Spark also provides a web GUI, which can be accessed at port 4040 or 4041.

Setting up history server for Spark:
The logs location can be configured in SPARK_HOME/conf/spark-defaults.conf. 
Whatever directory we give here needs to be created manually, otherwise it will throw a FileNotFoundException.
A sample configuration is in the github repo.
To start the history server, run $SPARK_HOME/sbin/start-history-server.sh
Visit <url>:18080 to see the history server UI.

=====================================================================================================================================================================

Resilient Distributed Dataset (RDD): https://sparkbyexamples.com/pyspark-rdd/
-> the basic building block of PySpark which is fault-tolerant, immutable distributed collection of objects.
-> RDD divides the data among the different nodes available - so parallelism is really good.
-> some of the basic operations on an RDD are: filter(), map(), persist() and pair RDD functions such as groupByKey(), join().
-> Lazy evaluation: PySpark does not evaluate the transformations and instead it keeps all the transformations till it sees the first RDD action.
-> By default, RDD partitions the elements to the number of cores available.

There are two ways to create an RDD:
	1. parallelizing an existing collection
		for e.g., create an RDD from a Python list (as we did in the first PySpark script).
		
	2. Referencing a dataset in an external storage system (HDFS, S3 etc)
	
Code level instructions are found in the GitHub repo.

Transformations and Actions

- transformations are all evaluated lazily, so they are not actually executed until an Action is encountered.
- Actions return the values from an RDD to a driver program. Any RDD function that returns non-RDD is considered an action.

Examples for transformations: map(), flatMap(), reduceByKey(), sortByKey(), filter().
Examples for actions: 
count(), first(), max(), 
reduce(), // accepts a function with which we can reduce all the records to a single - we can use this for count or sum
take(),   // Returns the record specified as an argument.
collect() // careful when using this with an RDD that contains huge amounts of data as we may run out of memory on the driver.
saveAsTextFile()

Shuffle operations:
 - shuffle is a mechanism used to redistribute the data across different executors and even across machines.
 - PySpark shuffling triggers when we perform certain transformation operations like groupByKey(), reduceByKey(), join() on RDDs.
 - shuffle is an expensive operation, because it needs disk I/O, data serialization & deserialization AND network I/O.
 
When we perform reduceByKey() operation, PySpark does the following

1. PySpark first runs map tasks on all partitions which groups all values for a single key.
2. The results of the map tasks are kept in memory.
3. When results do not fit in memory, PySpark stores the data into a disk.
4. PySpark shuffles the mapped data across partitions, some times it also stores the shuffled data into a disk for reuse when it needs to recalculate.
5. Run the garbage collection
6. Finally runs reduce tasks on each partition based on key.

Having too many partitions for very few data is not good since we can end up with many partitioned files with less number of records in each partition, which results in many tasks to process less data.
Vice versa, when we have too much data and very few partitions, it will result in longer running tasks and some times out of memory errors.

 ========================================================================================================================
||	Getting the right num of partitions is tricky: In case of performance issues in Spark, check the partitioning.		||
 ========================================================================================================================
 
 
Persistence and caching: https://blog.knoldus.com/understanding-persistence-in-apache-spark/

 - To store the results after transformations on an RDD, we can make use of caching and persistence.
 - With caching, it will always be in-memory of the worker nodes, whereas with persistence, we can store it in MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER,
   MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2
   
 - to understand the advantage of persistence, consider the example:
		transformation			transformation
   RDD1 ===============> RDD2 ===================> RDD3
   
   Now each time we do any kind of transformation on RDD3, RDD1 and RDD2 needs to be recomputed again.
   But, if we had cached RDD3, it wouldn't need to recompute this all over.
   
 - Persistence levels:
	Based on storage format:
   1. MEMORY_ONLY			: everything in memory
   2. DISK_ONLY				: everything in disk. useful if I/O to read/write this from disk is less expensive than recomputing
   3. MEMORY_AND_DISK		: Stores partitions in disk which do not fit in-memory. Also called spilling.
							  The data stored in disk is at /tmp.
							  
							  
	Based on memory format:
	
	Here serialized means storing in binary format. This is more space-efficient, but has the added cost of deserializing the data.
	
   4. MEMORY_ONLY_SER		: serialized in-memory.
   5. MEMORY_AND_DISK_SER	: serialized in-memory and disk.
   
   Based on partition replication:
   These options stores a replicated copy of the RDD into some other Worker Nodeâ€™s cache memory as well.
   Replicated data on the disk will be used to recreate the partition i.e. it helps to recompute the RDD if the other worker node goes down.
   
   6. DISK_ONLY_2
   7. MEMORY_AND_DISK_2
   8. MEMORY_ONLY_2
   9. MEMORY_AND_DISK_SER_2
   10.MEMORY_ONLY_SER_2
   
   
 - PySpark monitors all persist() and cache() calls that we make. It drops persisted data if not used or using a Least-Recently-Used algorithm.
 - We can also manually remove using unpersist().
 - unpersist() removes all blocks from memory and disk.