Apache Spark has the following main components:
 - Spark Core and RDDs: provides basic I/O, distributed task dispatching and scheduling.
 - Spark SQL: component lying on top of Spark Core, introduces SchemaRDD. Supports SQL with ODBC/JDBC server and command-line interfaces.
 - Spark Streaming: it leverages the fast scheduling capability of Spark Core, ingests data in small batches, and performs RDD transformations on them.
 - MLlib: distributed machine learning framework on top of Spark.
 
 
PySpark - Python API for Spark
https://sparkbyexamples.com/pyspark-tutorial/

Spark has a master-slave architecture, so whenever we run a Spark application, the master (Spark Driver) creates a "context" which is the entrypoint to the application, and all the actual operations (transformations and actions) are carried out on the worker nodes. The resources are managed by the Cluster Manager.

	   Spark Driver		 =====> Cluster Manager =====> 	Worker Node
  (creates Spark context)								task	task
  
  
There are different cluster managers that can be used:
Standalone, Apache Mesos, Hadoop YARN (most widely used) and Kubernetes

=> If we use "local" as cluster manager to run Spark on our laptops.

=====================================================================================================================================================================

Setting up Spark:
1. Install Java
	sudo amazon-linux-extras install java-openjdk11
	
2. Install Scala.
	curl -fL https://github.com/coursier/launchers/raw/master/cs-x86_64-pc-linux.gz | gzip -d > cs && chmod +x cs && ./cs setup
	
3. Install anaconda
	Prerequisites: yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver
	
	Installation steps: https://docs.anaconda.com/anaconda/install/linux/
	
	wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
	bash Anaconda3-2022.05-Linux-x86_64.sh
	Approve the steps and provide "yes" when asked about init.
	
4. Download and install Spark
	wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
	tar -zxvf spark-3.2.1-bin-hadoop3.2.tgz

	export SPARK_HOME=/home/ec2-user/spark-3.2.1-bin-hadoop3.2
	export PATH=$SPARK_HOME/bin:$PATH
	export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
	
When setting up Jupyter notebook on EC2, while running the "jupyter notebook" command, include a parameter "--ip=<EC2-public IP>".
The best process in my opinion is to include this in a "start_jupyter.sh" script. 
The above comands to set environment variables can also be included in this startup script.

To startup spark, after setting the environment variables, type "pyspark" in the terminal.
Spark also provides a web GUI, which can be accessed at port 4040 or 4041.

Setting up history server for Spark:
The logs location can be configured in SPARK_HOME/conf/spark-defaults.conf. 
Whatever directory we give here needs to be created manually, otherwise it will throw a FileNotFoundException.
A sample configuration is in the github repo.
To start the history server, run $SPARK_HOME/sbin/start-history-server.sh
Visit <url>:18080 to see the history server UI.

=====================================================================================================================================================================

Resilient Distributed Dataset (RDD): https://sparkbyexamples.com/pyspark-rdd/
-> the basic building block of PySpark which is fault-tolerant, immutable distributed collection of objects.
-> RDD divides the data among the different nodes available - so parallelism is really good.
-> some of the basic operations on an RDD are: filter(), map(), persist() and pair RDD functions such as groupByKey(), join().
-> Lazy evaluation: PySpark does not evaluate the transformations and instead it keeps all the transformations till it sees the first RDD action.
-> By default, RDD partitions the elements to the number of cores available.

There are two ways to create an RDD:
	1. parallelizing an existing collection
		for e.g., create an RDD from a Python list (as we did in the first PySpark script).
		
	2. Referencing a dataset in an external storage system (HDFS, S3 etc)
	
Code level instructions are found in the GitHub repo.

Transformations and Actions

- transformations are all evaluated lazily, so they are not actually executed until an Action is encountered.

Examples for transformations: map(), flatMap(), reduceByKey(), sortByKey(), filter().
Examples for actions: 
count(), first(), max(), 
reduce(), // accepts a function with which we can reduce all the records to a single - we can use this for count or sum
take(),   // Returns the record specified as an argument.
collect() // careful when using this with an RDD that contains huge amounts of data as we may run out of memory on the driver.
saveAsTextFile()